{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0adc4641",
   "metadata": {},
   "source": [
    "**Project Title:** *Predicting Short-Term Cryptocurrency Returns Using Machine Learning (G-Research Crypto Forecasting)*\n",
    " \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Domain Background**\n",
    "\n",
    "Cryptocurrency markets are volatile, nonlinear, and affected by strong cross-asset correlations.  Machine learning—especially gradient-boosting models like LightGBM—has become a popular approach in quantitative finance for modeling noisy, high-frequency financial data.\n",
    "\n",
    "This project uses the dataset accroding from the **G-Research Crypto Forecasting Kaggle competition** https://www.kaggle.com/competitions/g-research-crypto-forecasting , which provides minute-level OHLCV data and a future return “Target” for multiple crypto assets. The objective is to predict short-term price movements, a task that can meaningfully contribute to algorithmic trading strategies even when predictive correlations are small.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be8160",
   "metadata": {},
   "source": [
    "## **2. Problem Statement**\n",
    "\n",
    "The task is:\n",
    "\n",
    "> **To predict short-term cryptocurrency returns (the provided “Target”) using historical price data and engineered features.**\n",
    "\n",
    "Challenges include:\n",
    "\n",
    "* Highly noisy target values\n",
    "* Missing timestamps and asset-specific trading periods\n",
    "* The need to avoid data leakage\n",
    "* Non-stationary market behavior\n",
    "\n",
    "A successful predictive model must capture meaningful patterns while remaining robust to market noise.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4c5245",
   "metadata": {},
   "source": [
    "## **3. Datasets and Inputs**\n",
    "\n",
    "The dataset is create according   **G-Research Crypto Forecasting** Kaggle competition, but use the newest data. In this project, I use a **preprocessed version** (`df`) that contains:\n",
    "\n",
    "| Column      | Description                                       |\n",
    "| ----------- | ------------------------------------------------- |\n",
    "| `open_time` | Unix time (minute-level)                          |\n",
    "| `symbol`    | Identifier for each asset                         |\n",
    "| `close`     | Close price                                       | \n",
    "| `target`    | 15-minute future return (provided by competition) |\n",
    "\n",
    "\n",
    "The `target` is calculate according the method mentioned in the competition.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5565b1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open_time</th>\n",
       "      <th>close</th>\n",
       "      <th>symbol</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1761955200</td>\n",
       "      <td>1.076000</td>\n",
       "      <td>0GUSDT</td>\n",
       "      <td>-0.006433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1761955200</td>\n",
       "      <td>0.068070</td>\n",
       "      <td>AWEUSDT</td>\n",
       "      <td>-0.001617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1761955200</td>\n",
       "      <td>1.001000</td>\n",
       "      <td>SNXUSDT</td>\n",
       "      <td>-0.003002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1761955200</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>SLPUSDT</td>\n",
       "      <td>-0.001873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1761955200</td>\n",
       "      <td>0.165300</td>\n",
       "      <td>AXLUSDT</td>\n",
       "      <td>-0.004244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    open_time     close   symbol    target\n",
       "0  1761955200  1.076000   0GUSDT -0.006433\n",
       "1  1761955200  0.068070  AWEUSDT -0.001617\n",
       "2  1761955200  1.001000  SNXUSDT -0.003002\n",
       "3  1761955200  0.001069  SLPUSDT -0.001873\n",
       "4  1761955200  0.165300  AXLUSDT -0.004244"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv('mergee_df.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b661ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e6dd07",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "1. Align timestamps across assets\n",
    "2. Forward-fill missing Close data (limit = 60 minutes)\n",
    "3. Sort by timestamp\n",
    "4. Generate lag-based and market-relative features\n",
    "5. Remove rows with missing values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b549e358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data shape: (16329600, 4)\n",
      "Aligned data shape: (16323930, 4)\n",
      "Processed data shape: (16323930, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import mlflow\n",
    "import os\n",
    "\n",
    " \n",
    "\n",
    "df.rename(columns={'open_time':'timestamp','symbol':'Asset_ID','close':'Close','target':'Target'},inplace=True)\n",
    "\n",
    " \n",
    "#mlflow.set_tracking_uri('http://k8s-mlflow-mlflowin-4da4410a5a-161181029.ap-northeast-1.elb.amazonaws.com')\n",
    "#mlflow.set_experiment(\"crypto_lgbm_v1_mydata\")\n",
    "# ============================\n",
    "# Load Data\n",
    "# ============================\n",
    "use_cols = ['timestamp', 'Asset_ID', 'Close', 'Target']\n",
    "dtype_map = {'timestamp': 'int32', 'Asset_ID': 'string', 'Close': 'float32', 'Target': 'float32'}\n",
    "\n",
    "data = df[use_cols].astype(dtype_map)\n",
    "\n",
    "print(\"Raw data shape:\", data.shape)\n",
    "\n",
    "data.dropna(subset=['Close', 'Target'], inplace=True)\n",
    "\n",
    "# ============================\n",
    "# Align timestamps across assets\n",
    "# ============================\n",
    "asset_start_times = data.groupby('Asset_ID')['timestamp'].min()\n",
    "#global_start = asset_start_times.max()\n",
    "\n",
    "#if global_start > data['timestamp'].min():\n",
    "#    data = data[data['timestamp'] >= global_start].copy()\n",
    "\n",
    "print(\"Aligned data shape:\", data.shape)\n",
    "data.sort_values(['Asset_ID', 'timestamp'], inplace=True)\n",
    "\n",
    "# Forward fill limit = 60 minutes\n",
    "ffill_limit = 60\n",
    "processed_dfs = []\n",
    "\n",
    "for asset_id, df_asset in data.groupby('Asset_ID'):\n",
    "    df_asset = df_asset.sort_values('timestamp').set_index('timestamp')\n",
    "\n",
    "    full_index = np.arange(df_asset.index.min(),\n",
    "                           df_asset.index.max() + 60,\n",
    "                           60, dtype=np.int32)\n",
    "\n",
    "    df_asset = df_asset.reindex(full_index)\n",
    "\n",
    "    df_asset['Close'] = df_asset['Close'].ffill(limit=ffill_limit)\n",
    "    #df_asset['Target'] = df_asset['Target'].ffill(limit=ffill_limit)\n",
    "\n",
    "    df_asset.dropna(subset=['Close'], inplace=True)\n",
    "    df_asset['Asset_ID'] = asset_id\n",
    "    processed_dfs.append(df_asset.reset_index(names='timestamp'))\n",
    "\n",
    "data_proc = pd.concat(processed_dfs, ignore_index=True)\n",
    "data_proc.sort_values('timestamp', inplace=True)\n",
    "print(\"Processed data shape:\", data_proc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d37115",
   "metadata": {},
   "source": [
    "### Features Engineered\n",
    "\n",
    "* Lag returns: `return_1m`, `return_5m`, `return_15m`, `return_30m`, `return_60m`\n",
    "* Rolling trend indicators: `trend_15m`, `trend_60m`\n",
    "* Cross-sectional deviations: `diff_ret_1m`, … `diff_ret_60m`\n",
    "* Market price deviation: `diff_price` (optional)\n",
    "\n",
    "Final dataset size after processing:\n",
    "**~1.2–1.5 million rows** (depending on filtering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90337909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training data shape: (16301250, 18)\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Feature Engineering\n",
    "# ============================\n",
    "lag_list = [1, 5, 15, 30, 60]\n",
    "ma_window_list = [15, 60]\n",
    "\n",
    "data_proc['log_close'] = np.log(data_proc['Close'])\n",
    "\n",
    "\n",
    "data_proc['ret_15m'] = data_proc.groupby('Asset_ID')['Close'].transform(\n",
    "        lambda x: ( x.shift(-15)-x)/x\n",
    "    )\n",
    "\n",
    "# Log returns\n",
    "for L in lag_list:\n",
    "    data_proc[f'return_{L}m'] = data_proc.groupby('Asset_ID')['log_close'].transform(\n",
    "        lambda x: x - x.shift(L)\n",
    "    )\n",
    "\n",
    "# Rolling averages + trend\n",
    "for w in ma_window_list:\n",
    "    data_proc[f'avg_{w}m'] = data_proc.groupby('Asset_ID')['Close'].transform(\n",
    "        lambda x: x.shift(1).rolling(window=w).mean()\n",
    "    )\n",
    "    data_proc[f'trend_{w}m'] = np.log(data_proc['Close'] / data_proc[f'avg_{w}m'])\n",
    "\n",
    "# Market features\n",
    "for L in lag_list:\n",
    "    data_proc[f'mkt_ret_{L}m'] = data_proc.groupby('timestamp')[f'return_{L}m'].transform('mean')\n",
    "    data_proc[f'diff_ret_{L}m'] = data_proc[f'return_{L}m'] - data_proc[f'mkt_ret_{L}m']\n",
    "\n",
    "data_proc['mkt_close'] = data_proc.groupby('timestamp')['Close'].transform('mean')\n",
    "data_proc['diff_price'] = data_proc['Close'] - data_proc['mkt_close']\n",
    "\n",
    "# Drop intermediate columns\n",
    "drop_cols = ['log_close'] + \\\n",
    "            [f'avg_{w}m' for w in ma_window_list] + \\\n",
    "            [f'mkt_ret_{L}m' for L in lag_list] + \\\n",
    "            ['mkt_close']\n",
    "\n",
    "data_proc.drop(columns=drop_cols, inplace=True)\n",
    "#['diff_price'] +\n",
    "# [f'diff_ret_{L}m' for L in lag_list] +\n",
    "# [f'trend_{w}m' for w in ma_window_list]\n",
    "feature_cols = (\n",
    "    [f'trend_{w}m' for w in ma_window_list] +\n",
    "    [f'diff_ret_{L}m' for L in lag_list] +\n",
    "    [f'return_{L}m' for L in lag_list]  \n",
    "   \n",
    "   \n",
    ")\n",
    "\n",
    "data_proc.dropna(subset=feature_cols, inplace=True)\n",
    "print(\"Final training data shape:\", data_proc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d491fb9f",
   "metadata": {},
   "source": [
    "## **4. Proposed Solution**\n",
    "\n",
    "The proposed solution uses **LightGBM regression** to model the short-term price return (`Target`).\n",
    "Reasons for choosing LightGBM:\n",
    "\n",
    "* Handles nonlinear tabular patterns well\n",
    "* Efficient for large datasets\n",
    "* Supports early stopping and fast training\n",
    "* Popular in financial competitions\n",
    "\n",
    "### Model Pipeline\n",
    "\n",
    "1. **Preprocess data** (timestamp alignment, forward-fill, feature generation)\n",
    "2. **Use forward-chaining time-series cross-validation** (7 folds)\n",
    "3. **Train LightGBM models** with parameters:\n",
    "\n",
    "```\n",
    "learning_rate = 0.05  \n",
    "num_leaves = 256  \n",
    "n_estimators = 10000  \n",
    "early_stopping_rounds = 50  \n",
    "```\n",
    "\n",
    "4. **Evaluate models using correlation metric**\n",
    "5. **Select the best fold model**\n",
    "6. **Log everything to MLflow** (parameters, metrics, artifacts)\n",
    "\n",
    "The entire process simulates real-world forecasting and avoids data leakage.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591b158a",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "The primary metric is:\n",
    "\n",
    "## **Pearson Correlation Coefficient**\n",
    "\n",
    "[\n",
    "corr = \\frac{cov(y, \\hat{y})}{\\sigma_y \\sigma_{\\hat{y}}}\n",
    "]\n",
    "\n",
    "Reasons for using correlation:\n",
    "\n",
    "* Return magnitude is less important than directional accuracy\n",
    "* RMSE is not meaningful in noisy financial targets\n",
    "* Correlation is the official competition metric\n",
    "* Robust to scaling differences\n",
    "\n",
    "Secondary metrics:\n",
    "\n",
    "* Fold correlation values\n",
    "* Average correlation across all CV folds\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17da5069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation time from 2025-11-04 18:50:00 to 2025-11-08 12:40:00\n",
      "Fold 1: train=2037420, val=2037420\n",
      "Fold 1 corr = 0.02918\n",
      "validation time from 2025-11-08 12:40:00 to 2025-11-12 06:30:00\n",
      "Fold 2: train=4074840, val=2037420\n",
      "Fold 2 corr = 0.04608\n",
      "validation time from 2025-11-12 06:30:00 to 2025-11-16 00:20:00\n",
      "Fold 3: train=6112260, val=2037420\n",
      "Fold 3 corr = 0.06579\n",
      "validation time from 2025-11-16 00:20:00 to 2025-11-19 18:10:00\n",
      "Fold 4: train=8149680, val=2037420\n",
      "Fold 4 corr = 0.05149\n",
      "validation time from 2025-11-19 18:10:00 to 2025-11-23 12:00:00\n",
      "Fold 5: train=10187100, val=2037420\n",
      "Fold 5 corr = 0.03359\n",
      "validation time from 2025-11-23 12:00:00 to 2025-11-27 05:50:00\n",
      "Fold 6: train=12224520, val=2037420\n",
      "Fold 6 corr = 0.07105\n",
      "validation time from 2025-11-27 05:50:00 to 2025-11-30 23:44:01\n",
      "Fold 7: train=14261940, val=2039310\n",
      "Fold 7 corr = 0.09794\n",
      "Average corr: 0.05644527123118832\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Prepare CV Splitting by Time\n",
    "# ============================\n",
    " \n",
    "\n",
    "X = data_proc[feature_cols]\n",
    "y = data_proc['Target'].values\n",
    "timestamps = data_proc['timestamp']\n",
    "unique_times = np.sort(data_proc['timestamp'].unique())\n",
    "\n",
    "\n",
    "\n",
    "n_folds = 7\n",
    "n_unique = len(unique_times)\n",
    "segment_len = n_unique // (n_folds + 1)\n",
    "\n",
    "# Time boundaries\n",
    "boundaries = [unique_times[0]]\n",
    "for i in range(1, n_folds + 1):\n",
    "    idx = min(i * segment_len, n_unique - 1)\n",
    "    boundaries.append(unique_times[idx])\n",
    "boundaries.append(unique_times[-1] + 1)\n",
    "\n",
    "# ============================\n",
    "# Training with MLflow Tracking\n",
    "# ============================\n",
    "#mlflow.end_run()\n",
    "#with mlflow.start_run():\n",
    "if 1:\n",
    "\n",
    "    #mlflow.log_param(\"model\", \"lightgbm\")\n",
    "    #mlflow.log_param(\"learning_rate\", 0.05)\n",
    "    #mlflow.log_param(\"num_leaves\", 256)\n",
    "    #mlflow.log_param(\"lags\", lag_list)\n",
    "    #mlflow.log_param(\"rolling_windows\", ma_window_list)\n",
    "    #mlflow.log_param(\"n_folds\", n_folds)\n",
    "    #mlflow.log_param(\"features\", feature_cols)\n",
    "    fold_scores = []\n",
    "    fold_models = []\n",
    "\n",
    "    for fold in range(n_folds):\n",
    "        train_start = boundaries[fold ]\n",
    "        val_start = boundaries[fold + 1]\n",
    "        val_end = boundaries[fold + 2]\n",
    "        print('validation time from', pd.to_datetime(val_start,unit='s'), 'to', pd.to_datetime(val_end,unit='s'))\n",
    "        val_mask = (timestamps >= val_start) & (timestamps < val_end)\n",
    "        train_mask =  ( timestamps < val_start)# & (timestamps >= train_start)  \n",
    "\n",
    "        X_train, y_train = X[train_mask], y[train_mask]\n",
    "        X_val, y_val = X[val_mask], y[val_mask]\n",
    "\n",
    "        print(f\"Fold {fold+1}: train={len(y_train)}, val={len(y_val)}\")\n",
    "\n",
    "        model = lgb.LGBMRegressor(\n",
    "            objective='regression',\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=256,\n",
    "            n_estimators=10000,\n",
    "            verbose=-1\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='l2',\n",
    "            callbacks=[lgb.early_stopping(50, verbose=False)]\n",
    "        )\n",
    "\n",
    "        y_pred = model.predict(X_val)\n",
    "        corr = np.corrcoef(y_val, y_pred)[0, 1]\n",
    "        fold_scores.append(corr)\n",
    "        \n",
    "        fold_models.append(model)  # ADD THIS LINE\n",
    "\n",
    "        print(f\"Fold {fold+1} corr = {corr:.5f}\")\n",
    "        #mlflow.log_metric(f\"fold_{fold+1}_corr\", float(corr))\n",
    "\n",
    "        # Log the model of this fold\n",
    "        #mlflow.lightgbm.log_model(model, artifact_path=f\"model_fold_{fold+1}\")\n",
    "\n",
    "    best_idx = int(np.argmax(fold_scores))\n",
    "    best_model = fold_models[best_idx]\n",
    "\n",
    "    #   \"model\"，  MLflow registry  \n",
    "    #mlflow.lightgbm.log_model(best_model, artifact_path=\"model\")\n",
    "\n",
    "\n",
    "\n",
    "    avg_corr = float(sum(fold_scores) / len(fold_scores))\n",
    "    print(\"Average corr:\", avg_corr)\n",
    "    #mlflow.log_metric(\"avg_corr\", avg_corr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e038f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06c1e858",
   "metadata": {},
   "source": [
    "## **5. Benchmark Model**\n",
    "\n",
    "To determine whether the ML model adds value, we compare it to two simple baselines.\n",
    "\n",
    "### **Benchmark 1 — Zero Prediction**\n",
    "\n",
    "Predict future return = 0.\n",
    "\n",
    "Expected correlation: **≈ 0**\n",
    "\n",
    "### **Benchmark 2 — Copy Last Return**\n",
    "\n",
    "Predict:\n",
    "\n",
    "```\n",
    "ŷ_t = return_1m(t)\n",
    "```\n",
    "\n",
    "Expected correlation: **≈ 0.01 – 0.015**\n",
    "\n",
    "Any model achieving **> 0.02 correlation** is considered meaningful in this domain.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded99789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation time from 2025-11-04 18:50:00 to 2025-11-08 12:40:00\n",
      "Fold 1: train=2037420, val=2037420\n",
      "Fold 1 corr = -0.02023\n",
      "validation time from 2025-11-08 12:40:00 to 2025-11-12 06:30:00\n",
      "Fold 2: train=4074840, val=2037420\n",
      "Fold 2 corr = -0.03279\n",
      "validation time from 2025-11-12 06:30:00 to 2025-11-16 00:20:00\n",
      "Fold 3: train=6112260, val=2037420\n",
      "Fold 3 corr = -0.03735\n",
      "validation time from 2025-11-16 00:20:00 to 2025-11-19 18:10:00\n",
      "Fold 4: train=8149680, val=2037420\n",
      "Fold 4 corr = -0.04934\n",
      "validation time from 2025-11-19 18:10:00 to 2025-11-23 12:00:00\n",
      "Fold 5: train=10187100, val=2037420\n",
      "Fold 5 corr = -0.03055\n",
      "validation time from 2025-11-23 12:00:00 to 2025-11-27 05:50:00\n",
      "Fold 6: train=12224520, val=2037420\n",
      "Fold 6 corr = -0.04817\n",
      "validation time from 2025-11-27 05:50:00 to 2025-11-30 23:44:01\n",
      "Fold 7: train=14261940, val=2039310\n",
      "Fold 7 corr = -0.04992\n",
      "Average corr: 0.009055709222220801\n"
     ]
    }
   ],
   "source": [
    " \n",
    "### **Benchmark 2 — Copy Last Return**\n",
    "fold_models = []\n",
    "\n",
    "for fold in range(n_folds):\n",
    "    train_start = boundaries[fold ]\n",
    "    val_start = boundaries[fold + 1]\n",
    "    val_end = boundaries[fold + 2]\n",
    "    print('validation time from', pd.to_datetime(val_start,unit='s'), 'to', pd.to_datetime(val_end,unit='s'))\n",
    "    val_mask = (timestamps >= val_start) & (timestamps < val_end)\n",
    "    train_mask =  ( timestamps < val_start)# & (timestamps >= train_start)  \n",
    "\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_val, y_val = X[val_mask], y[val_mask]\n",
    "\n",
    "    print(f\"Fold {fold+1}: train={len(y_train)}, val={len(y_val)}\")\n",
    "\n",
    "        \n",
    "\n",
    "    y_pred =  X_val['return_1m']\n",
    "    corr = np.corrcoef(y_val, y_pred)[0, 1]\n",
    "    fold_scores.append(corr)\n",
    "    \n",
    "    \n",
    "\n",
    "    print(f\"Fold {fold+1} corr = {corr:.5f}\")\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "avg_corr = float(sum(fold_scores) / len(fold_scores))\n",
    "print(\"Average corr:\", avg_corr)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121a49b",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "\n",
    "This project applies machine learning techniques to a real-world financial forecasting problem. By combining:\n",
    "\n",
    "* Feature engineering\n",
    "* Time-series validation\n",
    "* Gradient-boosted models\n",
    "* MLflow experiment tracking\n",
    "\n",
    "the project aims to build a reproducible, production-grade forecasting pipeline. Even a small improvement in predictive correlation can be valuable in algorithmic trading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68a737d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
